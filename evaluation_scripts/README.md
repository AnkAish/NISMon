# Evaluation Scripts and Results

This folder contains scripts and data for evaluating trained classifiers (Random Forest, SVM, MLP) on test datasets, along with sample output results for the Random Forest model.

---

## ğŸ“ Directory Structure

```plain
evaluation_script/
â”œâ”€â”€ evaluation_NISMon_model.py    # Evaluation script for NISMon models
â”œâ”€â”€ merge_and_label_CSV_files.py  # Labels and merges generated metrics CSVs for evaluation
â”œâ”€â”€ dataset_testing.csv           # Test dataset (features and labels)
â””â”€â”€ evaluation_result_RF/         # Sample output directory for Random Forest evaluation
    â”œâ”€â”€ confusion_matrix.csv      # Raw confusion matrix values
    â”œâ”€â”€ confusion_matrix.png      # Confusion matrix heatmap
    â”œâ”€â”€ latency_resources.txt     # Latency vs. resource usage summary
    â”œâ”€â”€ metrics_summary.csv       # Summary statistics (precision, recall, F1-score)
    â”œâ”€â”€ pr_all_classes.png        # Precisionâ€“Recall curves for all classes
    â””â”€â”€ roc_all_classes.png       # ROC curves for all classes
```

---

## ğŸ“ Overview

1. **`evaluation_NISMon_model.py`**

   * Loads a trained classifier (RF, SVM, or MLP) from a `.joblib` file.
   * Loads the test dataset `dataset_testing.csv`.
   * Computes evaluation metrics: confusion matrix, classification report, ROC curves, PR curves, and latency/resource usage analysis.
   * Saves outputs to a results directory specified by the user.

2. **`merge_and_label_CSV_files.py`**

   * Reads metrics CSVs generated by `metrics_collection_with_random_faults.sh` or other collection scripts.
   * Labels each record with scenario and bandwidth.
   * Concatenates into a single DataFrame for evaluation input.

3. **`dataset_testing.csv`**

   * Contains the feature columns matching training data and the ground-truth label column.

4. **`evaluation_result_RF/`**

   * Provides an example of all output artifacts generated by running `evaluation_NISMon_model.py` with the Random Forest model.

---

## âš™ï¸ Prerequisites

* **Python 3.8+**
* Install required packages:

  ```bash
  pip install pandas scikit-learn matplotlib seaborn joblib
  ```
* A trained model file (e.g., `random_forest_model.joblib`, `svm_model.joblib`, or `mlp_model.joblib`) placed in this folder or specify its path.

---

## ğŸš€ Usage

1. **Prepare the environment**

   ```bash
   pip install -r requirements.txt
   ```

   Include at least:

   ```text
   pandas
   scikit-learn
   matplotlib
   seaborn
   joblib
   ```

2. **Merge and label metrics (optional)**
   If you need to prepare a combined metrics CSV from collection scripts:

   ```bash
   python merge_and_label_CSV_files.py
   ```

   This will generate `merged_labeled_periodic_fault_data.csv` in the current folder.

3. **Run evaluation**

   ```bash
   python evaluation_NISMon_model.py \
     --model-path <path/to/model.joblib> \
     --test-data dataset_testing.csv \
     --results-dir evaluation_result_RF
   ```

   * `--model-path`: Path to the trained model `.joblib` file.
   * `--test-data`: Path to the test dataset CSV.
   * `--results-dir`: Directory where evaluation outputs will be saved.

4. **Inspect results**
   Check the specified `results-dir` for:

   * `confusion_matrix.csv` and `confusion_matrix.png`
   * `metrics_summary.csv`
   * `pr_all_classes.png`, `roc_all_classes.png`
   * `latency_resources.txt`

---

## ğŸ“‚ Example Commands

```bash
# Evaluate Random Forest model
python evaluation_NISMon_model.py \
  --model-path ../classifier_model_scripts/random_forest_model.joblib \
  --test-data dataset_testing.csv \
  --results-dir evaluation_result_RF
```

```bash
# Evaluate SVM model
python evaluation_NISMon_model.py \
  --model-path ../classifier_model_scripts/svm_model.joblib \
  --test-data dataset_testing.csv \
  --results-dir evaluation_result_SVM
```

---

## ğŸ›  Customization

* **Adjust plotting parameters** in `evaluation_NISMon_model.py` for figure size or style.
* **Add new metrics** (e.g., confusion matrix normalization) by extending the script.
* **Benchmark latency/resources**: modify how `latency_resources.txt` is computed or formatted.

---
