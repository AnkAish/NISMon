# Classifier Model Scripts

This folder contains three standalone Python scripts that train and persist machine-learning classifiers. They consume preâ€‘processed experimental data (generated by `metrics_collector` suite) and save trained models to disk.

---

## ğŸ“ Repository Structure

```plain
classifier_model_scripts/
â”œâ”€â”€ random_forest_model.py   # Train and save a Random Forest classifier
â”œâ”€â”€ svm_model.py             # Train and save a Support Vector Machine classifier
â””â”€â”€ mlp_model.py             # Train and save a Multi-Layer Perceptron classifier
```

---

## ğŸ“ Overview

In a controlled experiment, your dataâ€‘collection pipeline (`metrics_collector`) recorded a set of features and labels. These scripts each load the resulting dataset, fit a model, and serialize the trained classifier so you can use it later for inference or evaluation.

---

## âš™ï¸ Prerequisites

* **Python 3.8+**
* **pip** (or **conda**) to install dependencies

### Python packages

```bash
pip install -r requirements.txt
```

If you donâ€™t already have one, create a `requirements.txt` in your project root containing at least:

```
pandas
scikit-learn
joblib
```

---

## ğŸš€ Usage

1. **Prepare your dataset**
   Ensure that your controlledâ€‘experiment script (`metrics_collector`) has generated a CSV dataset. The file should include:

   * Feature columns (e.g., PCIRdCur,ItoM,ItoMCacheNear,WiL,MemRead,MemWrite,MemTotal,drop_pct(%),CPU_busy(%),ksoft_avg,ksoft_max)
   * A target label column (for classification)

2. **Place the dataset**
   Copy or symlink your dataset file into this folder (or note its relative path).

3. **Run a model script**
   Each script accepts two arguments:

   ```bash
   python <model_script>.py \
     --input <path/to/dataset.csv> \
     --output-dir <path/to/save/models>
   ```

   * `--input`
     Path to your CSV or pickle file.

   * `--output-dir`
     Directory where the trained model artifact (e.g. `.joblib` file) will be saved. Defaults to the current working directory if omitted.

#### Examples

* **Random Forest**

  ```bash
  python random_forest_model.py \
    --input ../metrics_data/experiment1.csv \
    --output-dir ./models
  ```

* **Support Vector Machine**

  ```bash
  python svm_model.py \
    --input ../metrics_data/experiment1.csv
  ```

* **Multi-Layer Perceptron**

  ```bash
  python mlp_model.py \
    --input ../metrics_data/experiment1.csv \
    --output-dir ./models
  ```

---

## ğŸ” Whatâ€™s Inside Each Script

* **Data loading & preprocessing**
  Reads the dataset (CSV or pickle), splits into features (`X`) and labels (`y`), and performs any required scaling or encoding.

* **Model training**

  * `random_forest_model.py`
    Trains a `RandomForestClassifier` with default hyperparameters (you can customize inside the script).
  * `svm_model.py`
    Trains an `SVC` (Support Vector Machine).
  * `mlp_model.py`
    Trains an `MLPClassifier` (neural network).

* **Model serialization**
  Uses `joblib.dump()` to write the trained model to `<output-dir>/<model_name>.joblib`.

---

## ğŸ“‚ Output

After running, youâ€™ll find one file per script in your output directory, for example:

```plain
./models/
â”œâ”€â”€ random_forest_model.joblib
â”œâ”€â”€ svm_model.joblib
â””â”€â”€ mlp_model.joblib
```

Load these back in your inference pipeline:

```python
from joblib import load

clf = load("random_forest_model.joblib")
preds = clf.predict(X_new)
```
